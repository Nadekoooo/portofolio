{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8858762,"sourceType":"datasetVersion","datasetId":5333216},{"sourceId":8869696,"sourceType":"datasetVersion","datasetId":5338000},{"sourceId":8944254,"sourceType":"datasetVersion","datasetId":5382112},{"sourceId":8950879,"sourceType":"datasetVersion","datasetId":5386627}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# RISTEK Datathon 2024","metadata":{}},{"cell_type":"markdown","source":"### Notebook by CCC : Rahardi Salim, Vincent Davis Leonard Tjoeng, Christian Yudistira Hermawan\n### [University of Indonesia](https://www.ui.ac.id/)","metadata":{}},{"cell_type":"markdown","source":"## Table of contents\n\n1. Introduction\n\n2. Problem Domain\n\n3. Import Required Library and Dataset\n\n6. Step 1: Checking the data\n\n7. Step 2: Tidying the data\n\n8. Step 3: Exploratory analysis\n\n9. Step 4: Feature Engineering\n\n10. Step 5: Modeling\n\n10. Step 6: Reproducibility (Feature Importance)\n\n11. Conclusions","metadata":{}},{"cell_type":"markdown","source":"### 1. Introductions\n\nWelcome to the RISTEK Datathon 2024! This competition aims to develop a machine learning model to detect fraud among users of a fintech platform. In the current digital era, fraud detection is crucial for maintaining trust and security in financial transactions. By participating in this competition, we aim to build a robust model that can accurately identify fraudulent activities, ensuring the platform's integrity and security.","metadata":{}},{"cell_type":"markdown","source":"### 2. Problem Domain\n\n#### Dataset Description\nThe dataset for this competition is derived from financial product loan records of a fintech company. It includes various user features and loan activities. The dataset comprises the following files:\n\n1. **train.csv**: Contains the training data with user features and the target label for classification.\n   - `user_id`: Unique identifier for each user.\n   - `pc[0-16]`: Anonymized user identity features.\n   - `label`: Target variable (0: Non-fraud; 1: Fraud).\n\n2. **loan_activities.csv**: Records of financial product loans.\n   - `user_id`: Unique identifier for each user.\n   - `reference_contact`: Emergency contact provided by the user.\n   - `loan_type`: Type of loan taken by the user.\n   - `ts`: Timestamp of the loan creation.\n\n3. **non_borrower_user.csv**: Data of users who rarely take loans and are not the primary focus of classification.\n   - `user_id`: Unique identifier for each user.\n   - `pc[0-16]`: Anonymized user identity features.\n\n4. **test.csv**: Data for prediction submission.\n   - `user_id`: Unique identifier for each user matching the `sample_submission.csv`.\n   - `pc[0-16]`: Anonymized user identity features.\n\n5. **sample_submission.csv**: Example submission format.\n   - `user_id`: Unique identifier for each user matching the `test.csv`.\n   - `label`: Target variable (0: Non-fraud; 1: Fraud).\n\n#### Problem Statement\nFraud detection involves identifying user actions that qualify as fraudulent. In this competition, a fraudulent user is defined as someone who has taken a financial product loan but has not made the repayment by the due date. The objective is to develop a machine learning model to accurately detect such users.\n\n#### Evaluation Criteria\nThe model performance will be evaluated based on the Average Precision with `average='macro'`. The competition also emphasizes analysis, data processing, modeling, and notebook structure. The scoring breakdown is as follows:\n\n- **Private Leaderboard**: 25%\n- **Analysis**: 15%\n- **Data Processing**: 25%\n- **Modeling**: 30%\n- **Notebook Structure**: 5%","metadata":{}},{"cell_type":"markdown","source":"### 3. Import Required Library and Dataset","metadata":{}},{"cell_type":"markdown","source":"#### 3.1 Import Dataset","metadata":{}},{"cell_type":"code","source":"!pip install gdown\n\nimport os\nimport gdown\nimport zipfile\nimport logging\nfrom genericpath import isdir\n\ndef download_data(url, filename, dir_name=\"data\"):\n    if not os.path.isdir(dir_name):\n        os.mkdir(dir_name)\n    os.chdir(dir_name)\n    logging.info(\"Downloading data....\")\n    gdown.download(url, quiet=False)\n    logging.info(\"Extracting zip file....\")\n    with zipfile.ZipFile(f\"{filename}.zip\", 'r') as zip_ref:\n        zip_ref.extractall(filename)\n    os.remove(f\"{filename}.zip\")\n    os.chdir(\"..\")\n\ndownload_data(url=\"https://drive.google.com/uc?&id=1joOspf-LvEBdKLw48S2WeBno_l5J1DPj\",\n              filename=\"ristek-datathon-2024\",\n              dir_name=\"datathon-2024\")","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:07:17.746779Z","iopub.execute_input":"2024-07-14T15:07:17.747515Z","iopub.status.idle":"2024-07-14T15:07:39.560133Z","shell.execute_reply.started":"2024-07-14T15:07:17.747480Z","shell.execute_reply":"2024-07-14T15:07:39.559277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.2 Import Library","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\nimport math\nfrom geopy.geocoders import GoogleV3\nimport time\nimport optuna\nfrom catboost import CatBoostRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy.stats import mode\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import (RandomForestRegressor, AdaBoostRegressor, \n                              GradientBoostingRegressor, ExtraTreesRegressor)\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.neighbors import KNeighborsRegressor","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:07:39.562285Z","iopub.execute_input":"2024-07-14T15:07:39.562851Z","iopub.status.idle":"2024-07-14T15:07:44.048012Z","shell.execute_reply.started":"2024-07-14T15:07:39.562815Z","shell.execute_reply":"2024-07-14T15:07:44.047165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport itertools\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RepeatedStratifiedKFold\nfrom sklearn.svm import SVC\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom catboost import CatBoostClassifier\n\nfrom mlxtend.classifier import StackingCVClassifier\nimport shap\nfrom xgboost import XGBClassifier","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:07:44.049085Z","iopub.execute_input":"2024-07-14T15:07:44.049681Z","iopub.status.idle":"2024-07-14T15:07:48.856234Z","shell.execute_reply.started":"2024-07-14T15:07:44.049653Z","shell.execute_reply":"2024-07-14T15:07:48.855366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 1: Checking the Data","metadata":{}},{"cell_type":"code","source":"# Loading the datasets\ndf_train = pd.read_csv(\"/kaggle/working/datathon-2024/ristek-datathon-2024/ristek-datathon-2024/train.csv\", index_col=False)\ndf_test = pd.read_csv(\"/kaggle/working/datathon-2024/ristek-datathon-2024/ristek-datathon-2024/test.csv\", index_col=False)\ndf_sub = pd.read_csv(\"/kaggle/working/datathon-2024/ristek-datathon-2024/ristek-datathon-2024/sample_submission.csv\", index_col=False)\ndf_loan_activities = pd.read_csv(\"/kaggle/working/datathon-2024/ristek-datathon-2024/ristek-datathon-2024/loan_activities.csv\", index_col=False)\ndf_non_borrower_user = pd.read_csv(\"/kaggle/working/datathon-2024/ristek-datathon-2024/ristek-datathon-2024/non_borrower_user.csv\", index_col=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:07:48.858304Z","iopub.execute_input":"2024-07-14T15:07:48.858902Z","iopub.status.idle":"2024-07-14T15:07:55.540083Z","shell.execute_reply.started":"2024-07-14T15:07:48.858873Z","shell.execute_reply":"2024-07-14T15:07:55.539166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.1 Display the first few rows of the dataset","metadata":{}},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:07:55.541160Z","iopub.execute_input":"2024-07-14T15:07:55.541435Z","iopub.status.idle":"2024-07-14T15:07:55.576179Z","shell.execute_reply.started":"2024-07-14T15:07:55.541399Z","shell.execute_reply":"2024-07-14T15:07:55.575378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:07:55.577180Z","iopub.execute_input":"2024-07-14T15:07:55.577473Z","iopub.status.idle":"2024-07-14T15:07:55.600405Z","shell.execute_reply.started":"2024-07-14T15:07:55.577445Z","shell.execute_reply":"2024-07-14T15:07:55.599456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_loan_activities.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:07:55.601875Z","iopub.execute_input":"2024-07-14T15:07:55.602237Z","iopub.status.idle":"2024-07-14T15:07:55.612439Z","shell.execute_reply.started":"2024-07-14T15:07:55.602162Z","shell.execute_reply":"2024-07-14T15:07:55.611596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_non_borrower_user.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:07:55.613736Z","iopub.execute_input":"2024-07-14T15:07:55.614137Z","iopub.status.idle":"2024-07-14T15:07:55.635300Z","shell.execute_reply.started":"2024-07-14T15:07:55.614100Z","shell.execute_reply":"2024-07-14T15:07:55.634478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.2 Check data types and missing values","metadata":{}},{"cell_type":"code","source":"df_train.info()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:07:55.636445Z","iopub.execute_input":"2024-07-14T15:07:55.638136Z","iopub.status.idle":"2024-07-14T15:07:55.686474Z","shell.execute_reply.started":"2024-07-14T15:07:55.638112Z","shell.execute_reply":"2024-07-14T15:07:55.685593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.info()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:07:55.690277Z","iopub.execute_input":"2024-07-14T15:07:55.690590Z","iopub.status.idle":"2024-07-14T15:07:55.714508Z","shell.execute_reply.started":"2024-07-14T15:07:55.690566Z","shell.execute_reply":"2024-07-14T15:07:55.713632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_loan_activities.info()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:07:55.715507Z","iopub.execute_input":"2024-07-14T15:07:55.715829Z","iopub.status.idle":"2024-07-14T15:07:55.724945Z","shell.execute_reply.started":"2024-07-14T15:07:55.715805Z","shell.execute_reply":"2024-07-14T15:07:55.724095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_non_borrower_user.info()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:07:55.726196Z","iopub.execute_input":"2024-07-14T15:07:55.727339Z","iopub.status.idle":"2024-07-14T15:07:55.768549Z","shell.execute_reply.started":"2024-07-14T15:07:55.727306Z","shell.execute_reply":"2024-07-14T15:07:55.767621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.3 Summary statistics","metadata":{}},{"cell_type":"code","source":"df_train.describe()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:07:55.770187Z","iopub.execute_input":"2024-07-14T15:07:55.770876Z","iopub.status.idle":"2024-07-14T15:07:56.304799Z","shell.execute_reply.started":"2024-07-14T15:07:55.770838Z","shell.execute_reply":"2024-07-14T15:07:56.303820Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.describe()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:07:56.306053Z","iopub.execute_input":"2024-07-14T15:07:56.306521Z","iopub.status.idle":"2024-07-14T15:07:56.555018Z","shell.execute_reply.started":"2024-07-14T15:07:56.306487Z","shell.execute_reply":"2024-07-14T15:07:56.554112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_loan_activities.describe()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:07:56.556468Z","iopub.execute_input":"2024-07-14T15:07:56.556833Z","iopub.status.idle":"2024-07-14T15:07:57.063357Z","shell.execute_reply.started":"2024-07-14T15:07:56.556801Z","shell.execute_reply":"2024-07-14T15:07:57.062427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_non_borrower_user.describe()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:07:57.065326Z","iopub.execute_input":"2024-07-14T15:07:57.065720Z","iopub.status.idle":"2024-07-14T15:07:57.631642Z","shell.execute_reply.started":"2024-07-14T15:07:57.065690Z","shell.execute_reply":"2024-07-14T15:07:57.630730Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 2: Tidying the data","metadata":{}},{"cell_type":"markdown","source":"### Do we need to tidy the outliers? \n\nOutliers are data points that deviate significantly from the rest of the observations in a dataset. In many machine learning models, especially linear models, outliers can heavily influence the model's performance and predictions. Therefore, it's common practice to identify and handle outliers before training these models. However, this is not always necessary for tree-based models like Decision Trees, Random Forests, Gradient Boosting Machines (GBMs), and their variants (e.g., XGBoost, LightGBM, CatBoost).\n\nIn the modeling steps we will use robust tree decision model, so we will keep the outliers and also keep the data skewed","metadata":{}},{"cell_type":"markdown","source":"### Merging and adding some feature from df_loan_activity to df_train and df_test","metadata":{}},{"cell_type":"code","source":"df_loan_activities['loan_count'] = 1  # Tambahkan kolom untuk menghitung jumlah pinjaman\nloan_features = df_loan_activities.groupby('user_id').agg({\n    'loan_type': ['nunique', 'count'],  # Jumlah tipe pinjaman unik dan total pinjaman\n    'ts': ['min', 'max', 'mean', 'std'], # Waktu pinjaman pertama, terakhir, rata-rata, dan deviasi standar\n    'loan_count': 'sum'                 # Total jumlah pinjaman\n})\n\n# Flatten the column names\nloan_features.columns = ['loan_type_nunique', 'loan_type_count', 'loan_ts_min', 'loan_ts_max', 'loan_ts_mean', 'loan_ts_std', 'loan_count']\nloan_features.reset_index(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:07:57.632769Z","iopub.execute_input":"2024-07-14T15:07:57.633053Z","iopub.status.idle":"2024-07-14T15:08:00.407770Z","shell.execute_reply.started":"2024-07-14T15:07:57.633031Z","shell.execute_reply":"2024-07-14T15:08:00.406683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Membuat kolom in_loan_activities\ndf_train['in_loan_activities'] = df_train['user_id'].isin(df_loan_activities['user_id']).astype(int)\ndf_test['in_loan_activities'] = df_test['user_id'].isin(df_loan_activities['user_id']).astype(int)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:08:00.408880Z","iopub.execute_input":"2024-07-14T15:08:00.409520Z","iopub.status.idle":"2024-07-14T15:08:00.754056Z","shell.execute_reply.started":"2024-07-14T15:08:00.409486Z","shell.execute_reply":"2024-07-14T15:08:00.752961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df_train.merge(loan_features, on='user_id', how='left')\ndf_test = df_test.merge(loan_features, on='user_id', how='left')\n\n# Mengisi missing values yang mungkin muncul karena pengguna yang tidak ada dalam loan_activities\ndf_train.fillna(0, inplace=True)\ndf_test.fillna(0, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:08:00.755401Z","iopub.execute_input":"2024-07-14T15:08:00.755763Z","iopub.status.idle":"2024-07-14T15:08:01.216530Z","shell.execute_reply.started":"2024-07-14T15:08:00.755732Z","shell.execute_reply":"2024-07-14T15:08:01.215625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loan_activities_with_label = df_loan_activities.merge(df_train[['user_id', 'label']], left_on='reference_contact', right_on='user_id', how='left')","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:08:01.217589Z","iopub.execute_input":"2024-07-14T15:08:01.217858Z","iopub.status.idle":"2024-07-14T15:08:02.615905Z","shell.execute_reply.started":"2024-07-14T15:08:01.217836Z","shell.execute_reply":"2024-07-14T15:08:02.614927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Menghitung rata-rata fraud untuk setiap user_id di loan_activities\nfraud_avg = loan_activities_with_label.groupby('user_id_x')['label'].mean().reset_index()\nfraud_avg.columns = ['user_id', 'reference_fraud_avg']\ndf_train = df_train.merge(fraud_avg, on='user_id', how='left')\ndf_test = df_test.merge(fraud_avg, on='user_id', how='left')","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:08:02.617282Z","iopub.execute_input":"2024-07-14T15:08:02.617669Z","iopub.status.idle":"2024-07-14T15:08:03.949494Z","shell.execute_reply.started":"2024-07-14T15:08:02.617638Z","shell.execute_reply":"2024-07-14T15:08:03.948449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mengisi nilai yang hilang dengan 0\ndf_train['reference_fraud_avg'].fillna(-999, inplace=True)\ndf_test['reference_fraud_avg'].fillna(-999, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:08:03.950914Z","iopub.execute_input":"2024-07-14T15:08:03.951304Z","iopub.status.idle":"2024-07-14T15:08:03.965012Z","shell.execute_reply.started":"2024-07-14T15:08:03.951254Z","shell.execute_reply":"2024-07-14T15:08:03.964266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:08:03.966143Z","iopub.execute_input":"2024-07-14T15:08:03.966498Z","iopub.status.idle":"2024-07-14T15:08:04.256794Z","shell.execute_reply.started":"2024-07-14T15:08:03.966467Z","shell.execute_reply":"2024-07-14T15:08:04.255858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:08:04.257936Z","iopub.execute_input":"2024-07-14T15:08:04.258225Z","iopub.status.idle":"2024-07-14T15:08:04.393960Z","shell.execute_reply.started":"2024-07-14T15:08:04.258202Z","shell.execute_reply":"2024-07-14T15:08:04.392980Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 3: Exploratory Data Analysis (EDA)","metadata":{}},{"cell_type":"code","source":"# Visualize the target variable distribution\nsns.countplot(x='label', data=df_train)\nplt.title('Fraud vs Non-Fraud Distribution')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:08:04.395333Z","iopub.execute_input":"2024-07-14T15:08:04.395766Z","iopub.status.idle":"2024-07-14T15:08:04.704552Z","shell.execute_reply.started":"2024-07-14T15:08:04.395732Z","shell.execute_reply":"2024-07-14T15:08:04.703661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correlation matrix for train dataset\nplt.figure(figsize=(20, 18))\nsns.heatmap(df_train.corr(), annot=True, fmt=\".2f\")\nplt.title('Correlation Matrix for Train Dataset')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:08:04.705954Z","iopub.execute_input":"2024-07-14T15:08:04.706334Z","iopub.status.idle":"2024-07-14T15:08:08.639289Z","shell.execute_reply.started":"2024-07-14T15:08:04.706300Z","shell.execute_reply":"2024-07-14T15:08:08.638285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loan type distribution\nsns.countplot(y='loan_type', data=df_loan_activities)\nplt.title('Loan Type Distribution')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:08:08.640723Z","iopub.execute_input":"2024-07-14T15:08:08.641106Z","iopub.status.idle":"2024-07-14T15:08:09.197338Z","shell.execute_reply.started":"2024-07-14T15:08:08.641073Z","shell.execute_reply":"2024-07-14T15:08:09.196277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Analyze reference contacts in loan activities\nreference_contacts_count = df_loan_activities['reference_contact'].value_counts()\nprint(\"Top 10 Reference Contacts:\")\nprint(reference_contacts_count.head(10))","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:08:09.201843Z","iopub.execute_input":"2024-07-14T15:08:09.202134Z","iopub.status.idle":"2024-07-14T15:08:10.069576Z","shell.execute_reply.started":"2024-07-14T15:08:09.202110Z","shell.execute_reply":"2024-07-14T15:08:10.068400Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.1 Understanding Data Relationships and Detecting Anomalies","metadata":{}},{"cell_type":"markdown","source":"### 3.2 Automated Visualization with Pandas Profiling","metadata":{}},{"cell_type":"markdown","source":"After manually exploring the data, we will sum the EDA before and exploring more insight with this library","metadata":{}},{"cell_type":"code","source":"pip install ydata-profiling","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-07-14T15:00:16.020881Z","iopub.status.idle":"2024-07-14T15:00:16.021186Z","shell.execute_reply.started":"2024-07-14T15:00:16.021033Z","shell.execute_reply":"2024-07-14T15:00:16.021045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from ydata_profiling import ProfileReport","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:00:16.022434Z","iopub.status.idle":"2024-07-14T15:00:16.022791Z","shell.execute_reply.started":"2024-07-14T15:00:16.022600Z","shell.execute_reply":"2024-07-14T15:00:16.022614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"profile_train = ProfileReport(df_train, title = \"Traning Insight\")\nprofile_train.to_notebook_iframe()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:00:16.024027Z","iopub.status.idle":"2024-07-14T15:00:16.024339Z","shell.execute_reply.started":"2024-07-14T15:00:16.024178Z","shell.execute_reply":"2024-07-14T15:00:16.024191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"profile_test = ProfileReport(df_test, title = \"Test Insight\")\nprofile_test.to_notebook_iframe()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:00:16.025485Z","iopub.status.idle":"2024-07-14T15:00:16.025843Z","shell.execute_reply.started":"2024-07-14T15:00:16.025650Z","shell.execute_reply":"2024-07-14T15:00:16.025664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.3 Corr Matrix","metadata":{}},{"cell_type":"markdown","source":"We will see corr between features with spearman and pearson to decide which feature is highly correlated to the targeted feature. For this we need to truncate the categorical value on the dataset","metadata":{}},{"cell_type":"code","source":"train_dum = train_df.drop(columns = [\"city_or_regency\"])","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:00:16.026736Z","iopub.status.idle":"2024-07-14T15:00:16.027069Z","shell.execute_reply.started":"2024-07-14T15:00:16.026907Z","shell.execute_reply":"2024-07-14T15:00:16.026922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dum.info()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:00:16.028061Z","iopub.status.idle":"2024-07-14T15:00:16.028508Z","shell.execute_reply.started":"2024-07-14T15:00:16.028273Z","shell.execute_reply":"2024-07-14T15:00:16.028301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.3.1 Spearman Correlation","metadata":{}},{"cell_type":"code","source":"numeric_cols = train_dum.select_dtypes(include=np.number)\ncorr_matrix = numeric_cols.corr(method='spearman')\n\n# Plotting the heatmap\nplt.figure(figsize=(12, 8))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\nplt.title('Correlation Matrix')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:00:16.029618Z","iopub.status.idle":"2024-07-14T15:00:16.029974Z","shell.execute_reply.started":"2024-07-14T15:00:16.029806Z","shell.execute_reply":"2024-07-14T15:00:16.029821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.3.2 Pearson Correlation","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nnumeric_cols = train_dum.select_dtypes(include=np.number)\ncorr_matrix = numeric_cols.corr(method='pearson')\n\n# Plotting the heatmap\nplt.figure(figsize=(12, 8))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\nplt.title('Correlation Matrix')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:00:16.031337Z","iopub.status.idle":"2024-07-14T15:00:16.031662Z","shell.execute_reply.started":"2024-07-14T15:00:16.031501Z","shell.execute_reply":"2024-07-14T15:00:16.031515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From this corr matrix we will could see some interaction between feature. Notice how warmer color (red-ish) represent highly correlated feature","metadata":{}},{"cell_type":"markdown","source":"### 3.4 Missing Data Patterns","metadata":{}},{"cell_type":"markdown","source":"We will see the data if it is MCAR, MAR, MNAR","metadata":{}},{"cell_type":"code","source":"import plotly.graph_objs as go\nimport plotly.offline as pyo\nimport numpy as np\nimport pandas as pd\n\n# create a heatmap trace using plotly\ntrace = go.Heatmap(z=train_df.isnull().values.astype(int),\n                   colorscale='Viridis',\n                   showscale=True)\n\n# create a plot using plotly\nlayout = go.Layout(title='Heatmap of Missing Values',\n                   xaxis=dict(title='Columns'),\n                   yaxis=dict(title='Rows'))\nfig = go.Figure(data=[trace], layout=layout)\npyo.iplot(fig)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:00:16.032529Z","iopub.status.idle":"2024-07-14T15:00:16.032878Z","shell.execute_reply.started":"2024-07-14T15:00:16.032691Z","shell.execute_reply":"2024-07-14T15:00:16.032725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The heatmap of missing values reveals a concentration of missing data in specific columns, indicating that certain features have a high percentage of missing entries, while others are mostly complete. Additionally, the pattern of missingness does not follow any clear structure or block pattern, as missing values are scattered randomly across rows and columns. This randomness suggests that simpler imputation methods, such as mean or median imputation, might be sufficient to address the missing data.","metadata":{}},{"cell_type":"markdown","source":"#### 3.4.1 Imputing data","metadata":{}},{"cell_type":"code","source":"solid_waste_generated_mean = train_df['solid_waste_generated'].mean()\ngreen_open_space_mode = train_df['green_open_space'].mode()[0]\nsolid_waste_mode = train_df['solid_waste_generated'].mode()[0]\ntotal_landfills_mode =train_df['total_landfills'].mode()[0]\n\ndef impute_data(df):\n\n    # Set 'solid_waste_generated' to 0 where 'total_landfills' is NaN and 'solid_waste_generated' is also NaN\n    df.loc[df['total_landfills'].isna() & df['solid_waste_generated'].isna(), 'solid_waste_generated'] = 0\n\n    # Fill NaN in 'solid_waste_generated' with the mode where 'total_landfills' is not NaN\n    df.loc[df['total_landfills'].notna() & df['solid_waste_generated'].isna(), 'solid_waste_generated'] = solid_waste_mode\n\n    # Fill NaN in 'total_landfills' with the mode where 'solid_waste_generated' is not NaN\n    df.loc[df['solid_waste_generated'].notna() & df['total_landfills'].isna(), 'total_landfills'] = total_landfills_mode\n\n    # Fill the remaining NaN values in 'solid_waste_generated' with 0\n    df['solid_waste_generated'].fillna(0, inplace=True)\n    \n    # Fill the remaining NaN values in 'total_landfills' with 0\n    df['total_landfills'].fillna(0, inplace=True)\n\n    # Fill NaN values in 'green_open_space' with 0\n    df['green_open_space'].fillna(0, inplace=True)\n    \n    return df\n\n# Apply the imputation function to both train_df and test_df\ntrain_df = impute_data(train_df)\ntest_df = impute_data(test_df)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:00:16.034659Z","iopub.status.idle":"2024-07-14T15:00:16.034991Z","shell.execute_reply.started":"2024-07-14T15:00:16.034837Z","shell.execute_reply":"2024-07-14T15:00:16.034850Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The code is self explanatory. We fill the missing data to 0 if there are no landfills an solid waste data. Other than that we will impute the data with the mode.","metadata":{}},{"cell_type":"markdown","source":"## Step 4: Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"The feature engineering process includes the creation of several new features to enhance the dataset. demographic, and geographical aspects.","metadata":{}},{"cell_type":"markdown","source":"As mentioned on 3.1.2 we will create more insights from the city name. We convert it to latitude and langitude","metadata":{}},{"cell_type":"code","source":"api_key = 'AIzaSyDYJyK78qZkdfn-n0U8rpj3w1IxMMR-2SU'\ngeolocator = GoogleV3(api_key=api_key)\n\ndef geocode(city):\n    try:\n        location = geolocator.geocode(city + \", Indonesia\")\n        if location:\n            return location.latitude, location.longitude\n        else:\n            return None, None\n    except Exception as e:\n        print(f\"Error geocoding {city}: {e}\")\n        return None, None\n\n# Apply geocode function to city_or_regency column with rate limiting\ntrain_df['latitude'], train_df['longitude'] = zip(*train_df['city_or_regency'].apply(lambda x: geocode(x) if pd.notnull(x) else (None, None)))\ntime.sleep(1)  # Adding delay to respect rate limits\ntest_df['latitude'], test_df['longitude'] = zip(*test_df['city_or_regency'].apply(lambda x: geocode(x) if pd.notnull(x) else (None, None)))","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:00:16.036364Z","iopub.status.idle":"2024-07-14T15:00:16.036690Z","shell.execute_reply.started":"2024-07-14T15:00:16.036525Z","shell.execute_reply":"2024-07-14T15:00:16.036538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As the latitude and longitude created, we could see and plot the map for further analysis on the train data","metadata":{}},{"cell_type":"code","source":"import folium\n\ndef display_map_with_density_markers(data):\n    # Aggregate the data to count the number of occurrences of each location\n    location_counts = data.groupby(['latitude', 'longitude']).size().reset_index(name='id')\n\n    # Create a map centered at the first location in the DataFrame\n    map_object = folium.Map(location=[data.iloc[0]['latitude'], data.iloc[0]['longitude']], zoom_start=10)\n\n    # Add markers for each location in the aggregated DataFrame\n    for index, row in location_counts.iterrows():\n        latitude = row['latitude']\n        longitude = row['longitude']\n        location_name = data[(data['latitude'] == latitude) & (data['longitude'] == longitude)]['city_or_regency'].iloc[0]\n        if not pd.isnull(latitude) and not pd.isnull(longitude):  # Check if latitude and longitude are not NaN\n            # Adjust marker size based on the count of occurrences\n            folium.CircleMarker([latitude, longitude], popup=location_name, fill=True, fill_opacity=0.4).add_to(map_object)\n\n    # Display the map\n    return map_object\n\n# Assuming 'data' is your DataFrame with latitude and longitude columns\nmap_object = display_map_with_density_markers(train_df)\nmap_object.save('map_with_density_markers.html')  # Save the map as an HTML file\nmap_object\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:00:16.037783Z","iopub.status.idle":"2024-07-14T15:00:16.038089Z","shell.execute_reply.started":"2024-07-14T15:00:16.037935Z","shell.execute_reply":"2024-07-14T15:00:16.037947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The gdp_per_capita feature measures the average economic output per person in a city or regency by dividing the gross_regional_domestic_product by the population. The landfill/sampah feature calculates the amount of solid waste generated per landfill by dividing the solid_waste_generated by the total_landfills, providing insight into waste management efficiency. The density_category feature simplifies population density into three categories: low (density less than 500 people/km²), medium (density between 500 and 2000 people/km²), and high (density greater than 2000 people/km²), making it easier to interpret density differences. \n\nThe landfills_per_100k feature standardizes the number of landfills relative to the population size, offering a measure of waste management infrastructure. \n\nThe socioeconomic_index is a composite index that combines normalized values of the hdi, gdp_per_capita, and densities to capture overall socioeconomic status. \n\nFinally, the x, y, and z features transform geographic coordinates into Cartesian coordinates, making it easier to calculate spatial relationships and distances. These engineered features provide a more detailed and nuanced understanding of the dataset, enhancing the performance of machine learning models by incorporating economic, ","metadata":{}},{"cell_type":"code","source":"# Converting categorical features into numerical features\n# For 'traffic_density'\n\nlabel_enc = LabelEncoder()\ntrain_df[\"traffic_density\"] = label_enc.fit_transform(train_df['traffic_density'])\ntest_df[\"traffic_density\"] = label_enc.fit_transform(test_df['traffic_density'])","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:00:16.045116Z","iopub.status.idle":"2024-07-14T15:00:16.045483Z","shell.execute_reply.started":"2024-07-14T15:00:16.045311Z","shell.execute_reply":"2024-07-14T15:00:16.045327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nimport math\n\ndef geo_ekono_feat(df):\n    scaler = MinMaxScaler()\n    df[['hdi', 'gdp_per_capita', 'population_density']] = scaler.fit_transform(df[['hdi', 'gdp_per_capita', 'densities']])\n    df['socioeconomic_index'] = df[['hdi', 'gdp_per_capita', 'densities']].mean(axis=1)\n    \n    return df\n    \n    \ndef categorize_density(density):\n    if density < 500:\n        return 0\n    elif density < 2000:\n        return 1\n    else:\n        return 2\n\ndef capturing_latlang(df):\n    df[\"latitude\"] = df[\"latitude\"].apply(math.radians)\n    df[\"longitude\"] = df[\"longitude\"].apply(math.radians)\n    \n    df['x'] = df['latitude'].apply(math.cos) * df['longitude'].apply(math.cos)\n    df['y'] = df['latitude'].apply(math.cos) * df['longitude'].apply(math.sin)\n    df['z'] = df['latitude'].apply(math.sin)\n    \n    df.drop(columns = [\"latitude\", \"longitude\"], inplace = True)\n    \n    return df\n\ndef feature_eng(df):\n    df['gdp_per_capita'] = df['gross_regional_domestic_product'] / df['population']\n    df['landfill/sampah'] = df['solid_waste_generated'] / df[\"total_landfills\"]\n    df['density_category'] = df['densities'].apply(categorize_density)\n    df['landfills_per_100k'] = (df['total_landfills'] / df['population']) * 100000\n    \n    df_sorted = df.sort_values(by=['city_or_regency', 'year'])  # Sort the dataframe\n    \n    df_sorted['population_change'] = df_sorted.groupby('city_or_regency')['population'].pct_change()\n    df_sorted['hdi_change'] = df_sorted.groupby('city_or_regency')['hdi'].pct_change()\n    df_sorted['gdp_change'] = df_sorted.groupby('city_or_regency')['gross_regional_domestic_product'].pct_change()\n    \n    df_sorted['population_change'].fillna(0, inplace=True)\n    df_sorted['hdi_change'].fillna(0, inplace=True)\n    df_sorted['gdp_change'].fillna(0, inplace=True)\n    \n    # Merge the sorted results back into the original unsorted df by 'id'\n    df= pd.merge(df, df_sorted[['id', 'population_change', 'hdi_change', 'gdp_change']],\n                         on='id', how='left')\n    \n    df = geo_ekono_feat(df)\n    df = capturing_latlang(df)\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:00:16.047101Z","iopub.status.idle":"2024-07-14T15:00:16.047451Z","shell.execute_reply.started":"2024-07-14T15:00:16.047283Z","shell.execute_reply":"2024-07-14T15:00:16.047298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = feature_eng(train_df)\ntest_df = feature_eng(test_df)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:00:16.048914Z","iopub.status.idle":"2024-07-14T15:00:16.049247Z","shell.execute_reply.started":"2024-07-14T15:00:16.049084Z","shell.execute_reply":"2024-07-14T15:00:16.049098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For later modelling we will drop highly coor feature that we created here","metadata":{}},{"cell_type":"code","source":"train_df.drop(columns = ['socioeconomic_index', 'population_density', 'landfill/sampah', 'density_category'], inplace = True)\ntest_df.drop(columns = ['socioeconomic_index', 'population_density', 'landfill/sampah', 'density_category'], inplace = True)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:00:16.050773Z","iopub.status.idle":"2024-07-14T15:00:16.051107Z","shell.execute_reply.started":"2024-07-14T15:00:16.050945Z","shell.execute_reply":"2024-07-14T15:00:16.050959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 5: Modeling","metadata":{}},{"cell_type":"markdown","source":"### 5.1 Split data","metadata":{}},{"cell_type":"markdown","source":"Load data from the pre-processed data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/datathon-help/train_with_fraud_avg.csv\")\ntest = pd.read_csv(\"/kaggle/input/datathon-help/test_with_fraud_avg.csv\")\nloan_activities = pd.read_csv(\"/kaggle/input/ori-datathon-24/loan_activities.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:08:10.070887Z","iopub.execute_input":"2024-07-14T15:08:10.071295Z","iopub.status.idle":"2024-07-14T15:08:18.887687Z","shell.execute_reply.started":"2024-07-14T15:08:10.071264Z","shell.execute_reply":"2024-07-14T15:08:18.886851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Split data to X and y","metadata":{}},{"cell_type":"code","source":"X_train = train.drop(['user_id','label'],axis=1)\ny_train = train['label']\nX_test = test.drop(['user_id'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:08:18.888945Z","iopub.execute_input":"2024-07-14T15:08:18.889400Z","iopub.status.idle":"2024-07-14T15:08:18.964698Z","shell.execute_reply.started":"2024-07-14T15:08:18.889365Z","shell.execute_reply":"2024-07-14T15:08:18.963832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.2 Params","metadata":{}},{"cell_type":"markdown","source":"The parameters are based on tuning using optuna","metadata":{}},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:08:18.965866Z","iopub.execute_input":"2024-07-14T15:08:18.966215Z","iopub.status.idle":"2024-07-14T15:08:18.970787Z","shell.execute_reply.started":"2024-07-14T15:08:18.966170Z","shell.execute_reply":"2024-07-14T15:08:18.969855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\nclass_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:08:32.146439Z","iopub.execute_input":"2024-07-14T15:08:32.146818Z","iopub.status.idle":"2024-07-14T15:08:32.366609Z","shell.execute_reply.started":"2024-07-14T15:08:32.146789Z","shell.execute_reply":"2024-07-14T15:08:32.365528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_params_lgb_weight ={\n    'lambda_l1': 0.5752012128486822,\n    'lambda_l2': 0.5677230575819568,\n    'num_leaves': 33,\n    'feature_fraction': 0.9,\n    'bagging_fraction': 1.0,\n    'bagging_freq': 2,\n    'min_child_samples': 71,\n    'class_weight': class_weights_dict,  # Adding class weights\n    'device': 'gpu',  # Use GPU\n    'gpu_use_dp': True  # Use double precision\n    , 'random_state' : 42\n} #Best trial: 0.044426967000041645 weight = {0: 0.5064087731186884, 1: 39.50902643455835}\n\n#best weighted params\nbest_params_cb_weight = {'iterations': 333, 'depth': 5, 'learning_rate': 0.08285986576978271, 'l2_leaf_reg': 1.8188727288827022, 'border_count': 189, 'random_strength': 0.04589201773336512, 'bagging_temperature': 3.3517475268377224, 'od_type': 'IncToDec', 'od_wait': 25,\n    'class_weights': class_weights,  # Adding class weights\n    'task_type': 'GPU'  # Use GPU\n    , 'random_state' : 42\n} #Best trial: 0.04443454009809692 weight : {0: 0.5064087731186884, 1: 39.50902643455835}\n\nbest_params_xgb_weight = {'lambda': 9.14729918837456, 'alpha': 0.06394468748796704, 'colsample_bytree': 0.7, 'subsample': 0.9, 'learning_rate': 0.06108581565857493, 'n_estimators': 791, 'max_depth': 4, 'min_child_weight': 7}\n\n# Adding the missing parameters\nbest_params_xgb_weight['scale_pos_weight'] = class_weights_dict[1] / class_weights_dict[0]\nbest_params_xgb_weight['tree_method'] = 'hist' #Best trial: 0.04462405348434853, \nbest_params_xgb_weight['device'] = \"gpu\"","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:08:32.405862Z","iopub.execute_input":"2024-07-14T15:08:32.406443Z","iopub.status.idle":"2024-07-14T15:08:32.414409Z","shell.execute_reply.started":"2024-07-14T15:08:32.406396Z","shell.execute_reply":"2024-07-14T15:08:32.413496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lgb_params = {'lambda_l1': 0.0339770921696201, 'lambda_l2': 9.530796262405104, 'num_leaves': 65, 'feature_fraction': 0.7, 'bagging_fraction': 0.9, 'bagging_freq': 1, 'min_child_samples': 43}\n\n# cb_params = {'iterations': 831, 'depth': 6, 'learning_rate': 0.03338644432817077, 'l2_leaf_reg': 0.17320396497280668, 'border_count': 221, 'random_strength': 0.7071802263431674, 'bagging_temperature': 0.16496895778469275, 'od_type': 'Iter', 'od_wait': 41}\n\n# xg_params = {'lambda': 0.3077128191383428, 'alpha': 0.0012520196417128444, 'colsample_bytree': 0.7, 'subsample': 0.9, 'learning_rate': 0.02270602067576259, 'n_estimators': 861, 'max_depth': 5, 'min_child_weight': 9}\n\nrf_params = {\n    'max_depth': 15,\n    'min_samples_leaf': 8,\n    'random_state': 42\n}","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:08:34.961057Z","iopub.execute_input":"2024-07-14T15:08:34.961451Z","iopub.status.idle":"2024-07-14T15:08:34.966442Z","shell.execute_reply.started":"2024-07-14T15:08:34.961400Z","shell.execute_reply":"2024-07-14T15:08:34.965490Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.3 Classifier","metadata":{}},{"cell_type":"code","source":"RANDOM_SEED = 42","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:08:36.217087Z","iopub.execute_input":"2024-07-14T15:08:36.217469Z","iopub.status.idle":"2024-07-14T15:08:36.221545Z","shell.execute_reply.started":"2024-07-14T15:08:36.217440Z","shell.execute_reply":"2024-07-14T15:08:36.220624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below are the first-level (base) classifiers used in the stacking model. The outputs probability of theese first level models will be used by the second-level (meta) model to classify the data","metadata":{}},{"cell_type":"code","source":"cl1 = RandomForestClassifier(**rf_params)\ncl2 = DecisionTreeClassifier(max_depth = 5)\ncl3 = CatBoostClassifier(**best_params_cb_weight)\ncl4 = LGBMClassifier(**best_params_lgb_weight)\ncl5 = ExtraTreesClassifier(bootstrap=False, criterion='entropy', max_features=0.55, min_samples_leaf=8, min_samples_split=4, n_estimators=100) # Optimized using TPOT\ncl6 = XGBClassifier(**best_params_xgb_weight)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:08:39.433174Z","iopub.execute_input":"2024-07-14T15:08:39.433545Z","iopub.status.idle":"2024-07-14T15:08:39.442321Z","shell.execute_reply.started":"2024-07-14T15:08:39.433519Z","shell.execute_reply":"2024-07-14T15:08:39.441471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifiers = {\n    \"RandomForest\": cl1,\n    \"DecisionTree\": cl2,\n    \"CatBoost\": cl3,\n    \"LGBM\": cl4,\n    \"ExtraTrees\": cl5,\n    \"XGBoost\":cl6\n}","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:08:40.755979Z","iopub.execute_input":"2024-07-14T15:08:40.756359Z","iopub.status.idle":"2024-07-14T15:08:40.761208Z","shell.execute_reply.started":"2024-07-14T15:08:40.756330Z","shell.execute_reply":"2024-07-14T15:08:40.760230Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.4 Level 2 Classifier","metadata":{}},{"cell_type":"markdown","source":"Using Logistic Regression as Meta Model","metadata":{}},{"cell_type":"code","source":"mlr = LogisticRegression()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:08:42.946049Z","iopub.execute_input":"2024-07-14T15:08:42.946808Z","iopub.status.idle":"2024-07-14T15:08:42.950980Z","shell.execute_reply.started":"2024-07-14T15:08:42.946774Z","shell.execute_reply":"2024-07-14T15:08:42.949941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 6 : Training","metadata":{}},{"cell_type":"markdown","source":"### 6.1 Level 1 Classifiers","metadata":{}},{"cell_type":"markdown","source":"Fitting the level 1 classifiers","metadata":{}},{"cell_type":"code","source":"models_names = list() ","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:08:44.625708Z","iopub.execute_input":"2024-07-14T15:08:44.626479Z","iopub.status.idle":"2024-07-14T15:08:44.630567Z","shell.execute_reply.started":"2024-07-14T15:08:44.626441Z","shell.execute_reply":"2024-07-14T15:08:44.629553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\">>>> Training started <<<<\")\nfor key in classifiers:\n    classifier = classifiers[key]\n    models_names.append(key)\n    print(f\"{key} done!\")\n    classifier.fit(X_train, y_train)\n    classifiers[key] = classifier","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:08:45.312971Z","iopub.execute_input":"2024-07-14T15:08:45.313317Z","iopub.status.idle":"2024-07-14T15:18:05.785885Z","shell.execute_reply.started":"2024-07-14T15:08:45.313290Z","shell.execute_reply":"2024-07-14T15:18:05.784784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6.2 Meta Classifier (Log Reg)","metadata":{}},{"cell_type":"markdown","source":"Adding the pre-fitted model (in  6.1) to a list","metadata":{}},{"cell_type":"code","source":"used_model = ['RandomForest', 'DecisionTree', 'ExtraTrees', 'LGBM','CatBoost','XGBoost'] \nclassifier_exp = []\nfor label in used_model:\n        classifier = classifiers[label]\n        classifier_exp.append(classifier)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:18:05.787610Z","iopub.execute_input":"2024-07-14T15:18:05.788152Z","iopub.status.idle":"2024-07-14T15:18:05.793314Z","shell.execute_reply.started":"2024-07-14T15:18:05.788125Z","shell.execute_reply":"2024-07-14T15:18:05.792467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"using stacking CVC Classifier as the stacker ","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import make_scorer, average_precision_score\nfrom mlxtend.classifier import StackingCVClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\n# Define the average precision scorer\naverage_precision_scorer = make_scorer(average_precision_score, needs_proba=True)\n\n# # Assuming classifier_exp is defined and contains your classifiers\n# classifier_exp = [classifier1, classifier2, classifier3]  # Replace with actual classifiers\n\n# Define random seed\nRANDOM_SEED = 42\n\n# Initialize the StackingCVClassifier\nscl = StackingCVClassifier(classifiers=classifier_exp,\n                           meta_classifier=LogisticRegression(C=0.1), \n                           use_probas=True,  \n                           random_state=RANDOM_SEED)\n\n# # Perform cross-validation\n# scores = cross_val_score(scl, X_train, y_train, cv=2, scoring=average_precision_scorer)\n# print(\"Meta model (scl) - average precision: %0.5f \" % (scores.mean()))\n\n# Fit the model\nscl.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:54:08.824340Z","iopub.execute_input":"2024-07-14T15:54:08.824757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## Step 6: Prediction","metadata":{}},{"cell_type":"markdown","source":"After finetuning, stacking and modeling finally we will approach the final step. The Predictions!","metadata":{}},{"cell_type":"markdown","source":"### 7.1 Model Prediction","metadata":{}},{"cell_type":"code","source":"y_pred = scl.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:05:32.835761Z","iopub.status.idle":"2024-07-14T15:05:32.836106Z","shell.execute_reply.started":"2024-07-14T15:05:32.835946Z","shell.execute_reply":"2024-07-14T15:05:32.835959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7.2 Group average (reference)","metadata":{}},{"cell_type":"markdown","source":"Instead of directly submitting the prediction, we instead will look at the user_id highest fraud average based on reference. Because the test dataset has already been predicted, we will first merge the test and train dataset to get the new highest fraud average","metadata":{}},{"cell_type":"code","source":"test = test.merge(y_pred, on='user_id', how='left')\n\n# Menggabungkan dataset train dan test\ncombined = pd.concat([train, test], ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:05:32.837781Z","iopub.status.idle":"2024-07-14T15:05:32.838092Z","shell.execute_reply.started":"2024-07-14T15:05:32.837939Z","shell.execute_reply":"2024-07-14T15:05:32.837952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Getting the reference info from the loan_activities, as we dropped the reference_contact column before","metadata":{}},{"cell_type":"code","source":"loan_activities_with_pred = loan_activities.merge(combined[['user_id', 'pred']], left_on='reference_contact', right_on='user_id', how='left')\n\n# Menghitung rata-rata prediksi untuk setiap user_id di loan_activities\nfraud_avg_pred = loan_activities_with_pred.groupby('user_id_x')['pred'].max().reset_index()\nfraud_avg_pred.columns = ['user_id', 'reference_fraud_avg']\n\n# Menggabungkan kembali hasil ke dataset combined\ncombined = combined.merge(fraud_avg_pred, on='user_id', how='left')","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:05:32.838894Z","iopub.status.idle":"2024-07-14T15:05:32.839214Z","shell.execute_reply.started":"2024-07-14T15:05:32.839056Z","shell.execute_reply":"2024-07-14T15:05:32.839070Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7.3 Predict Based on Group Average","metadata":{}},{"cell_type":"markdown","source":"Some user_id doesn't exist in loan_activities.csv, so we will just use the fraud value in label column","metadata":{}},{"cell_type":"code","source":"combined['reference_fraud_avg'].fillna(combined['label'], inplace=True)\n\n# Mengkategorikan reference_fraud_avg > 0.5 sebagai fraud (label = 1)\ncombined['label'] = (combined['reference_fraud_avg'] > 0.5).astype(int)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:05:32.840468Z","iopub.status.idle":"2024-07-14T15:05:32.840922Z","shell.execute_reply.started":"2024-07-14T15:05:32.840678Z","shell.execute_reply":"2024-07-14T15:05:32.840713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Select the test label in the combined dataset","metadata":{}},{"cell_type":"code","source":"test_updated = combined[combined['user_id'].isin(test['user_id'])]","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:05:32.842405Z","iopub.status.idle":"2024-07-14T15:05:32.842873Z","shell.execute_reply.started":"2024-07-14T15:05:32.842617Z","shell.execute_reply":"2024-07-14T15:05:32.842636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7.4 Submission","metadata":{}},{"cell_type":"code","source":"sample = pd.read_csv(\"/kaggle/input/ori-datathon-24/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:00:16.083289Z","iopub.status.idle":"2024-07-14T15:00:16.083594Z","shell.execute_reply.started":"2024-07-14T15:00:16.083437Z","shell.execute_reply":"2024-07-14T15:00:16.083449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample['label'] = test_updated['label']","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:00:16.084771Z","iopub.status.idle":"2024-07-14T15:00:16.085097Z","shell.execute_reply.started":"2024-07-14T15:00:16.084940Z","shell.execute_reply":"2024-07-14T15:00:16.084953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample.to_csv(\"Submission_stacking_grouped.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-07-14T15:00:16.086285Z","iopub.status.idle":"2024-07-14T15:00:16.086614Z","shell.execute_reply.started":"2024-07-14T15:00:16.086450Z","shell.execute_reply":"2024-07-14T15:00:16.086464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusions","metadata":{}},{"cell_type":"markdown","source":"In this notebook, we successfully predicted the happiness score using a comprehensive approach that included Exploratory Data Analysis (EDA) and advanced modeling techniques such as stacking.\n\nThe use of stacking in our modeling process was a key component. Stacking allowed us to combine the strengths of multiple models to improve our predictions. By leveraging the predictions of several base models and using a meta-model to make the final predictions, we enhanced the accuracy and robustness of our happiness score predictions.\n\nOverall, the combination of thorough EDA, insightful feature engineering, and advanced stacking techniques led to a robust model capable of predicting happiness scores with high accuracy. This comprehensive approach underscores the importance of detailed data analysis and sophisticated modeling techniques in developing effective predictive models.","metadata":{}}]}